{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adadoun/inventoryPlanningRecommendation/blob/main/DataPreparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview:\n",
        "\n",
        "This notebook prepare the data to be used for model training and evaluation"
      ],
      "metadata": {
        "id": "cdkVFsnDIo4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library Import"
      ],
      "metadata": {
        "id": "NgPqjyiPKvLi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eoDLx0FeyM0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import hashlib\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPy1uy9Ue6_t",
        "outputId": "fe604d86-2cfc-43b9-a582-69575d75966d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GrIwjV5fVCI"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('drive/MyDrive/Collab_DATA/PolarData/sales_data_csv.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1s56YJhfi7G"
      },
      "outputs": [],
      "source": [
        "def hash_sku(sku: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a hashed identifier for a SKU.\n",
        "\n",
        "    Args:\n",
        "        sku (str): The original SKU string.\n",
        "\n",
        "    Returns:\n",
        "        str: An 8-character hash of the SKU.\n",
        "    \"\"\"\n",
        "    return hashlib.md5(str(sku).encode()).hexdigest()[:8]\n",
        "\n",
        "def get_top_skus(df: pd.DataFrame, threshold: float = 0.90) -> list:\n",
        "    \"\"\"\n",
        "    Identify the top SKUs that account for a given percentage of total sales.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing sales data.\n",
        "        threshold (float, optional): The cumulative sales percentage threshold. Defaults to 0.90.\n",
        "\n",
        "    Returns:\n",
        "        list: List of top SKUs.\n",
        "    \"\"\"\n",
        "    sku_sales = df.groupby('SKU')['QUANTITY_SOLD'].sum().sort_values(ascending=False)\n",
        "    cumulative_percentage = sku_sales.cumsum() / sku_sales.sum()\n",
        "    top_skus = cumulative_percentage[cumulative_percentage <= threshold].index.tolist()\n",
        "    print(f\"Number of SKUs representing {threshold*100}% of sales: {len(top_skus)}\")\n",
        "    print(f\"Total number of SKUs: {len(sku_sales)}\")\n",
        "    return top_skus\n",
        "\n",
        "def prepare_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepare the data for analysis by aggregating to weekly level and merging current inventory levels.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Raw sales data.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Prepared weekly sales data with current inventory levels.\n",
        "    \"\"\"\n",
        "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
        "    current_levels = df.groupby('SKU')['CURRENT_LEVEL'].first().reset_index()\n",
        "    weekly_sales = df.groupby(['SKU', pd.Grouper(key='DATE', freq='W-MON')])['QUANTITY_SOLD'].sum().reset_index()\n",
        "    weekly_sales = weekly_sales.sort_values(['SKU', 'DATE'])\n",
        "    weekly_sales = weekly_sales.merge(current_levels, on='SKU', how='left')\n",
        "    return weekly_sales\n",
        "\n",
        "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add time-based features to the dataset.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with a 'DATE' column.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with additional time-based features.\n",
        "    \"\"\"\n",
        "    df['year'] = df['DATE'].dt.year\n",
        "    df['month'] = df['DATE'].dt.month\n",
        "    df['week'] = df['DATE'].dt.isocalendar().week\n",
        "    df['day_of_week'] = df['DATE'].dt.dayofweek\n",
        "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
        "    df['quarter'] = df['DATE'].dt.quarter\n",
        "\n",
        "    # Cyclical encoding of time features\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
        "    df['week_sin'] = np.sin(2 * np.pi * df['week']/53)\n",
        "    df['week_cos'] = np.cos(2 * np.pi * df['week']/53)\n",
        "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week']/7)\n",
        "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week']/7)\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df: pd.DataFrame, lags: list = [1, 2, 3, 4, 5, 6, 12, 24, 36, 48, 52]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add lagged sales features to the dataset.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Sales data DataFrame.\n",
        "        lags (list, optional): List of lag periods to create. Defaults to [1, 2, 3, 4, 5, 6, 12, 24, 36, 48, 52].\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with additional lagged sales features.\n",
        "    \"\"\"\n",
        "    for lag in lags:\n",
        "        df[f'lag_{lag}'] = df.groupby('SKU')['QUANTITY_SOLD'].shift(lag)\n",
        "    return df\n",
        "\n",
        "def add_rolling_features(df: pd.DataFrame, windows: list = [1, 2, 3, 4, 5, 6, 12, 24, 36, 48, 52]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add rolling mean and standard deviation features to the dataset.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Sales data DataFrame.\n",
        "        windows (list, optional): List of window sizes for rolling calculations.\n",
        "                                  Defaults to [1, 2, 3, 4, 5, 6, 12, 24, 36, 48, 52].\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with additional rolling features.\n",
        "    \"\"\"\n",
        "    df = df.sort_values(['SKU', 'DATE'])\n",
        "    for window in windows:\n",
        "        df[f'rolling_mean_{window}'] = df.groupby('SKU')['QUANTITY_SOLD'].transform(\n",
        "            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
        "        )\n",
        "        df[f'rolling_std_{window}'] = df.groupby('SKU')['QUANTITY_SOLD'].transform(\n",
        "            lambda x: x.shift(1).rolling(window=window, min_periods=1).std()\n",
        "        )\n",
        "    return df\n",
        "\n",
        "def fill_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fill missing values in lag and rolling features.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with lag and rolling features.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with missing values filled.\n",
        "    \"\"\"\n",
        "    df = df.sort_values(['SKU', 'DATE'])\n",
        "    lag_columns = [col for col in df.columns if col.startswith('lag_')]\n",
        "    rolling_columns = [col for col in df.columns if col.startswith('rolling_')]\n",
        "\n",
        "    df_filled = df.copy()\n",
        "\n",
        "    for col in lag_columns + rolling_columns:\n",
        "        df_filled[col] = df_filled.groupby('SKU')[col].ffill()\n",
        "        df_filled[col] = df_filled[col].fillna(0)\n",
        "\n",
        "    return df_filled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation and Filtering"
      ],
      "metadata": {
        "id": "jpa3c6pw-Yic"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbU4FexQfbx2",
        "outputId": "a514fc54-9170-487a-f7a4-c997a8c73acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of SKUs representing 90.0% of sales: 815\n",
            "Total number of SKUs: 3835\n"
          ]
        }
      ],
      "source": [
        "# Apply hash function to SKU column\n",
        "df['SKU'] = df['SKU'].apply(hash_sku)\n",
        "\n",
        "weekly_sales = prepare_data(df)\n",
        "\n",
        "# Get top SKUs\n",
        "top_skus = get_top_skus(weekly_sales, threshold=0.90)\n",
        "\n",
        "# Filter for top SKUs\n",
        "weekly_sales = weekly_sales[weekly_sales['SKU'].isin(top_skus)]\n",
        "\n",
        "weekly_sales = weekly_sales.replace([np.inf, -np.inf], np.nan).dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "WpeeoLtr-bWb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CApIZ2laggAi"
      },
      "outputs": [],
      "source": [
        "# feature engineering steps\n",
        "weekly_sales = add_time_features(weekly_sales)\n",
        "weekly_sales = add_lag_features(weekly_sales)\n",
        "weekly_sales = add_rolling_features(weekly_sales)\n",
        "\n",
        "# After feature engineering, apply the function:\n",
        "weekly_sales = fill_missing_values(weekly_sales)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity Checks on the data"
      ],
      "metadata": {
        "id": "e80-iWmB-jkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6tpIUyVnjfP",
        "outputId": "254a6a62-6acc-4608-8dab-52fa793c1e25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after handling missing values: (177112, 49)\n",
            "\n",
            "Final check for NaN or infinite values:\n",
            "Number of missing values: 0\n",
            "Number of infinite values: 0\n"
          ]
        }
      ],
      "source": [
        "# If there are still NaN values, you might want to drop those rows\n",
        "weekly_sales = weekly_sales.dropna()\n",
        "\n",
        "print(\"Shape after handling missing values:\", weekly_sales.shape)\n",
        "\n",
        "# Final check\n",
        "print(\"\\nFinal check for NaN or infinite values:\")\n",
        "# Check for any remaining NaN values\n",
        "print(f\"Number of missing values: {weekly_sales.isna().sum().sum()}\")\n",
        "print(f\"Number of infinite values: {np.isinf(weekly_sales.select_dtypes(include=np.number)).sum().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the data into train/test\n",
        "The idea is to keep the last three months of data as test set for each sku"
      ],
      "metadata": {
        "id": "Dmi7La06-p1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Temporal split (last 3 months as test set)\n",
        "test_start_date = weekly_sales['DATE'].max() - pd.DateOffset(months=3)\n",
        "train_data = weekly_sales[weekly_sales['DATE'] < test_start_date]\n",
        "test_data = weekly_sales[weekly_sales['DATE'] >= test_start_date]"
      ],
      "metadata": {
        "id": "wk0zgKDCespR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of train data: {train_data.shape}\")\n",
        "print(f\"Shape of test data: {test_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ2EXEk-lgpv",
        "outputId": "1a075c83-3e4a-48cd-94cf-5664660978c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train data: (167164, 49)\n",
            "Shape of test data: (9948, 49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use only common skus in the test set"
      ],
      "metadata": {
        "id": "RNmtifpO-2qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get common SKUs\n",
        "common_skus = set(train_data['SKU']) & set(test_data['SKU'])\n",
        "\n",
        "# Filter train and test data to include only common SKUs\n",
        "train_data = train_data[train_data['SKU'].isin(common_skus)]\n",
        "test_data = test_data[test_data['SKU'].isin(common_skus)]\n",
        "\n",
        "# Verify that all SKUs in train data are also in test data\n",
        "assert set(train_data['SKU']) == set(test_data['SKU']), \"SKUs in train and test data do not match\"\n",
        "\n",
        "print(f\"Number of common SKUs: {len(common_skus)}\")\n",
        "print(f\"Shape of train data: {train_data.shape}\")\n",
        "print(f\"Shape of test data: {test_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3SgGTbZldFW",
        "outputId": "5e0196c2-a697-4243-d342-ecab43107b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of common SKUs: 792\n",
            "Shape of train data: (166552, 49)\n",
            "Shape of test data: (9862, 49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assign an index for each SKU to be used later as input of the NN"
      ],
      "metadata": {
        "id": "LbMv3ALp_CN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sku_to_index = {sku: idx for idx, sku in enumerate(train_data['SKU'].unique())}\n",
        "train_data['SKU_INDEX'] = train_data['SKU'].map(sku_to_index)\n",
        "test_data['SKU_INDEX'] = test_data['SKU'].map(sku_to_index)"
      ],
      "metadata": {
        "id": "RRBPVyjeUWzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to_csv('drive/MyDrive/Collab_DATA/PolarData/train_data.csv', index=False)\n",
        "test_data.to_csv('drive/MyDrive/Collab_DATA/PolarData/test_data.csv', index=False)"
      ],
      "metadata": {
        "id": "mZgvFe-PeuMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yDrmHv8GUcFz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaa0Ot4/YaGZgIlXR9avOW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}